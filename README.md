# Awesome In-Context DiT

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

A curated list of research papers that leverage in-context learning techniques in Diffusion Transformers (DiT) for various downstream tasks.

## Contents

- [Introduction](#introduction)
- [Papers](#papers)
  - [Image Generation](#image-generation)
  - [Text-to-Image](#text-to-image)
  - [Image Editing](#image-editing)
  - [Video Generation](#video-generation)
  - [3D Generation](#3d-generation)
  - [Other Applications](#other-applications)
- [Resources](#resources)
- [Contributing](#contributing)
- [License](#license)

## Introduction

Diffusion Transformers (DiT) have emerged as powerful models for generative tasks, combining the strengths of diffusion models with transformer architectures. In-context learning techniques allow these models to adapt to new tasks without parameter updates by conditioning on relevant examples or prompts.

This repository aims to collect and organize research exploring in-context learning capabilities within DiT models across various downstream applications.

## Papers

### Image Generation

- **Paper Title** (Conference/Journal YYYY) [[Paper](link)] [[Code](link)] [[Project Page](link)]  
  *Author 1, Author 2, Author 3*  
  Summary of the paper's approach and contributions.

### Text-to-Image

- **Paper Title** (Conference/Journal YYYY) [[Paper](link)] [[Code](link)] [[Project Page](link)]  
  *Author 1, Author 2, Author 3*  
  Summary of the paper's approach and contributions.

### Image Editing

- **Paper Title** (Conference/Journal YYYY) [[Paper](link)] [[Code](link)] [[Project Page](link)]  
  *Author 1, Author 2, Author 3*  
  Summary of the paper's approach and contributions.

### Video Generation

- **Paper Title** (Conference/Journal YYYY) [[Paper](link)] [[Code](link)] [[Project Page](link)]  
  *Author 1, Author 2, Author 3*  
  Summary of the paper's approach and contributions.

### 3D Generation

- **Paper Title** (Conference/Journal YYYY) [[Paper](link)] [[Code](link)] [[Project Page](link)]  
  *Author 1, Author 2, Author 3*  
  Summary of the paper's approach and contributions.

### Other Applications

- **Paper Title** (Conference/Journal YYYY) [[Paper](link)] [[Code](link)] [[Project Page](link)]  
  *Author 1, Author 2, Author 3*  
  Summary of the paper's approach and contributions.

## Resources

- [Original DiT Paper](https://arxiv.org/abs/2212.09748) - Scalable Diffusion Models with Transformers
- [DiT GitHub Repository](https://github.com/facebookresearch/DiT)
- [Hugging Face DiT Models](https://huggingface.co/models?search=DiT)

## Contributing

We welcome contributions! Please feel free to submit a PR to add new papers or resources.

**Guidelines for adding papers:**
1. Ensure the paper uses in-context learning techniques with DiT models
2. Follow the established format for paper entries
3. Place the paper in the appropriate category
4. Provide links to paper, code, and project page when available

## License

MIT License
